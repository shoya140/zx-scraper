{
 "metadata": {
  "name": "",
  "signature": "sha256:27de6356bb8bbe898d04a5c680563932a072c06bdf95d891c5b92acb771a6ad2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#coding: utf-8\n",
      "\n",
      "import json\n",
      "import urllib\n",
      "from bs4 import BeautifulSoup\n",
      "from multiprocessing import Pool\n",
      "\n",
      "base_url = 'http://zexy.net'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean(string):\n",
      "    return string.strip().replace('\\n', '').replace('\\t', '').encode('utf-8')\n",
      "\n",
      "def scrape(url):\n",
      "    html = urllib.urlopen(url)\n",
      "    soup = BeautifulSoup(html)\n",
      "    contents = soup.find('div', attrs={'id':'contents'})\n",
      "    if not contents:\n",
      "        return\n",
      "    \n",
      "    # main\n",
      "    main = soup.find('article',attrs={'id':'h1_box'})\n",
      "    title = main.find('h1')\n",
      "    description = main.find('p')\n",
      "    data = {\n",
      "        'url': url,\n",
      "        'title': clean(title.text),\n",
      "        'description': clean(description.text)\n",
      "    }\n",
      "    image_section = main.find('section', attrs={'class':'main_img'})\n",
      "    if image_section:\n",
      "        image = image_section.find('img')\n",
      "        data.update({'image': clean(base_url + image['src'])})\n",
      "\n",
      "    # tags\n",
      "    tag = soup.find('ul', attrs={'class':'tag_icon'})\n",
      "    tags = tag.findAll('li')\n",
      "    data.update({'tags':[clean(x.text) for x in tags]})\n",
      "    \n",
      "    # articles\n",
      "    articles = soup.findAll('article', attrs={'class':['', 'no_s']})\n",
      "    da = []\n",
      "    for article in articles:\n",
      "        d = {}\n",
      "        title = article.find('h2')\n",
      "        if title:\n",
      "            d.update({'title':clean(title.text)})\n",
      "        image = article.find('img')\n",
      "        if image: \n",
      "            d.update({'image': clean(base_url + image['src'])})\n",
      "        text = article.find('p', attrs={'class':'item1_text'})\n",
      "        if text: \n",
      "            d.update({'text':clean(text.text)})\n",
      "        column = article.find('section', attrs={'class':'column'})\n",
      "        if column: \n",
      "            d.update({'column':clean(column.text)})\n",
      "        da.append(d)\n",
      "    data.update({'articles':da})\n",
      "\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urls = ['http://zexy.net/article/app%09d' %i for i in range(0, 100)]\n",
      "pl = Pool()\n",
      "%time sc = pl.map(scrape, urls)\n",
      "data = [s for s in sc if s]\n",
      "pl.terminate()\n",
      "print \"exported %d articles\" % len(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 10.4 ms, sys: 4.05 ms, total: 14.4 ms\n",
        "Wall time: 6.86 s\n",
        "exported 92 articles\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('out.json','w')\n",
      "f.write(json.dumps(data, ensure_ascii=False, indent=2))\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}